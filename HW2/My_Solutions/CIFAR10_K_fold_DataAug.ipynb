{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_K-fold-DataAug.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnitaKirkovska/Machine_Learning_Class/blob/master/HW2/My_Solutions/CIFAR10_K_fold_DataAug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "5DXAMMmrsO9K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading the data**"
      ]
    },
    {
      "metadata": {
        "id": "jtUvg1Vlrupy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K-gzJGKPlFwa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Preprocess the data**"
      ]
    },
    {
      "metadata": {
        "id": "PcSfcxhelLV7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "train_images = train_images.astype('float32')/255\n",
        "test_images = test_images.astype('float32')/255\n",
        "\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xs9Iy4auuuO8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**The Architecture**"
      ]
    },
    {
      "metadata": {
        "id": "KU_InkfGuwBc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "import keras\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=3,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "datagen.fit(train_images)\n",
        "\n",
        "opt = keras.optimizers.rmsprop(lr=0.0005, decay=1e-6)\n",
        "\n",
        "\n",
        "def build_model():\n",
        "  model = models.Sequential()\n",
        "  \n",
        "  model.add(layers.Conv2D(64, (3,3), activation='relu', input_shape=(32,32,3)))\n",
        "  model.add(layers.MaxPooling2D(2,2))\n",
        "\n",
        "  model.add(layers.Conv2D(128, (3,3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2,2))\n",
        "\n",
        "  model.add(layers.Conv2D(256, (3,3), activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(2,2))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "\n",
        "  model.add(layers.Dense(10, activation='softmax'))\n",
        "  model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FpB2h-Nyubnp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**K-fold validation**"
      ]
    },
    {
      "metadata": {
        "id": "wdn-9EDJuGtq",
        "colab_type": "code",
        "outputId": "468799d3-3ccc-45f0-de19-c7287cc940cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8602
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "\n",
        "k = 5\n",
        "\n",
        "num_val_samples = len(train_images)//k\n",
        "all_histories = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('processing fold #', i)\n",
        "  val_images = train_images[i* num_val_samples: (i+1) * num_val_samples]\n",
        "  val_labels = train_labels[i* num_val_samples: (i+1) * num_val_samples]\n",
        "  \n",
        "  partial_train_images = np.concatenate(\n",
        "    [train_images[:i * num_val_samples],\n",
        "    train_images[(i+1)* num_val_samples:]],\n",
        "    axis=0)\n",
        "  \n",
        "  partial_train_labels = np.concatenate(\n",
        "    [train_labels[:i * num_val_samples],\n",
        "    train_labels[(i+1)* num_val_samples:]],\n",
        "    axis=0)\n",
        "  \n",
        "  model = build_model()\n",
        "  history = model.fit_generator(datagen.flow(partial_train_images, \n",
        "                                           partial_train_labels, \n",
        "                                           batch_size=32),\n",
        "                                           epochs=50,\n",
        "                                           steps_per_epoch=1250,\n",
        "                                           validation_data=(val_images, val_labels),\n",
        "                                           validation_steps=312)\n",
        "  k_history = history.history['val_acc']\n",
        "  all_histories.append(k_history)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.7910 - acc: 0.3432 - val_loss: 1.4261 - val_acc: 0.4916\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.5065 - acc: 0.4616 - val_loss: 1.3189 - val_acc: 0.5284\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.3591 - acc: 0.5175 - val_loss: 1.3537 - val_acc: 0.5410\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.2674 - acc: 0.5543 - val_loss: 1.0973 - val_acc: 0.6132\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1923 - acc: 0.5809 - val_loss: 1.0122 - val_acc: 0.6477\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 1.1326 - acc: 0.6049 - val_loss: 0.9280 - val_acc: 0.6804\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0829 - acc: 0.6204 - val_loss: 0.9063 - val_acc: 0.6853\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0362 - acc: 0.6402 - val_loss: 0.8552 - val_acc: 0.7028\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0058 - acc: 0.6490 - val_loss: 0.8799 - val_acc: 0.6942\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9691 - acc: 0.6653 - val_loss: 0.8984 - val_acc: 0.7052\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.9389 - acc: 0.6771 - val_loss: 0.7464 - val_acc: 0.7414\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9120 - acc: 0.6831 - val_loss: 0.8038 - val_acc: 0.7205\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8983 - acc: 0.6920 - val_loss: 0.8757 - val_acc: 0.7063\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8785 - acc: 0.6987 - val_loss: 0.7637 - val_acc: 0.7381\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8560 - acc: 0.7048 - val_loss: 0.7160 - val_acc: 0.7574\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8465 - acc: 0.7081 - val_loss: 0.9125 - val_acc: 0.7052\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8262 - acc: 0.7165 - val_loss: 0.8078 - val_acc: 0.7347\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8135 - acc: 0.7219 - val_loss: 0.7903 - val_acc: 0.7335\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7992 - acc: 0.7257 - val_loss: 0.7634 - val_acc: 0.7464\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7909 - acc: 0.7321 - val_loss: 0.7229 - val_acc: 0.7641\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7751 - acc: 0.7349 - val_loss: 0.7356 - val_acc: 0.7648\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7715 - acc: 0.7357 - val_loss: 0.7659 - val_acc: 0.7551\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7526 - acc: 0.7436 - val_loss: 0.6763 - val_acc: 0.7755\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7541 - acc: 0.7410 - val_loss: 0.7049 - val_acc: 0.7662\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7400 - acc: 0.7477 - val_loss: 0.6667 - val_acc: 0.7721\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7333 - acc: 0.7491 - val_loss: 0.6880 - val_acc: 0.7782\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7362 - acc: 0.7473 - val_loss: 0.6740 - val_acc: 0.7774\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7252 - acc: 0.7510 - val_loss: 0.6824 - val_acc: 0.7808\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7180 - acc: 0.7572 - val_loss: 0.7248 - val_acc: 0.7690\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7118 - acc: 0.7582 - val_loss: 0.7071 - val_acc: 0.7736\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7070 - acc: 0.7600 - val_loss: 0.6658 - val_acc: 0.7831\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7075 - acc: 0.7617 - val_loss: 0.7740 - val_acc: 0.7544\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6949 - acc: 0.7652 - val_loss: 0.7540 - val_acc: 0.7624\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6952 - acc: 0.7660 - val_loss: 0.7612 - val_acc: 0.7595\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6899 - acc: 0.7681 - val_loss: 0.7344 - val_acc: 0.7667\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6763 - acc: 0.7716 - val_loss: 0.6655 - val_acc: 0.7869\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6818 - acc: 0.7678 - val_loss: 0.6303 - val_acc: 0.7935\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6759 - acc: 0.7715 - val_loss: 0.7242 - val_acc: 0.7760\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6693 - acc: 0.7751 - val_loss: 0.8123 - val_acc: 0.7556\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6787 - acc: 0.7727 - val_loss: 0.7061 - val_acc: 0.7764\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6745 - acc: 0.7749 - val_loss: 0.6716 - val_acc: 0.7889\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6737 - acc: 0.7736 - val_loss: 0.6319 - val_acc: 0.8032\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6681 - acc: 0.7747 - val_loss: 0.6877 - val_acc: 0.7817\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6694 - acc: 0.7759 - val_loss: 0.6623 - val_acc: 0.7864\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6721 - acc: 0.7731 - val_loss: 0.6777 - val_acc: 0.7820\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6755 - acc: 0.7731 - val_loss: 0.6540 - val_acc: 0.7880\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6748 - acc: 0.7742 - val_loss: 0.6564 - val_acc: 0.7897\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6708 - acc: 0.7758 - val_loss: 0.6265 - val_acc: 0.7974\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6678 - acc: 0.7761 - val_loss: 0.7657 - val_acc: 0.7792\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.6707 - acc: 0.7750 - val_loss: 0.7217 - val_acc: 0.7858\n",
            "processing fold # 1\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 33s 26ms/step - loss: 1.7667 - acc: 0.3518 - val_loss: 1.5526 - val_acc: 0.4313\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.4887 - acc: 0.4655 - val_loss: 1.2318 - val_acc: 0.5656\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.3487 - acc: 0.5212 - val_loss: 1.1524 - val_acc: 0.6054\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.2499 - acc: 0.5618 - val_loss: 1.2103 - val_acc: 0.5856\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.1772 - acc: 0.5864 - val_loss: 1.0367 - val_acc: 0.6412\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.1076 - acc: 0.6138 - val_loss: 0.9745 - val_acc: 0.6620\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0499 - acc: 0.6340 - val_loss: 0.9334 - val_acc: 0.6732\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0072 - acc: 0.6481 - val_loss: 0.9220 - val_acc: 0.6835\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9790 - acc: 0.6602 - val_loss: 0.8858 - val_acc: 0.6988\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9416 - acc: 0.6763 - val_loss: 0.8586 - val_acc: 0.7079\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9129 - acc: 0.6868 - val_loss: 0.8799 - val_acc: 0.7034\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8826 - acc: 0.6954 - val_loss: 0.7942 - val_acc: 0.7361\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8658 - acc: 0.7024 - val_loss: 0.7610 - val_acc: 0.7403\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8475 - acc: 0.7072 - val_loss: 0.7589 - val_acc: 0.7410\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8253 - acc: 0.7151 - val_loss: 0.7512 - val_acc: 0.7507\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.8141 - acc: 0.7208 - val_loss: 0.7674 - val_acc: 0.7448\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7987 - acc: 0.7253 - val_loss: 0.6969 - val_acc: 0.7650\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7864 - acc: 0.7315 - val_loss: 0.7224 - val_acc: 0.7603\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7753 - acc: 0.7335 - val_loss: 0.7143 - val_acc: 0.7592\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7584 - acc: 0.7412 - val_loss: 0.7066 - val_acc: 0.7676\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7506 - acc: 0.7431 - val_loss: 0.6888 - val_acc: 0.7728\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7369 - acc: 0.7476 - val_loss: 0.7237 - val_acc: 0.7627\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7332 - acc: 0.7501 - val_loss: 0.7216 - val_acc: 0.7668\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7313 - acc: 0.7513 - val_loss: 0.6924 - val_acc: 0.7685\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7224 - acc: 0.7551 - val_loss: 0.6498 - val_acc: 0.7906\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.7063 - acc: 0.7599 - val_loss: 0.6332 - val_acc: 0.7916\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7019 - acc: 0.7599 - val_loss: 0.7458 - val_acc: 0.7616\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6938 - acc: 0.7630 - val_loss: 0.6628 - val_acc: 0.7875\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 0.7003 - acc: 0.7630 - val_loss: 0.6778 - val_acc: 0.7781\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6862 - acc: 0.7686 - val_loss: 0.7437 - val_acc: 0.7662\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6861 - acc: 0.7665 - val_loss: 0.6973 - val_acc: 0.7757\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6790 - acc: 0.7709 - val_loss: 0.6875 - val_acc: 0.7791\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6769 - acc: 0.7720 - val_loss: 0.7520 - val_acc: 0.7710\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6725 - acc: 0.7741 - val_loss: 0.6883 - val_acc: 0.7823\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6691 - acc: 0.7752 - val_loss: 0.6449 - val_acc: 0.7906\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6603 - acc: 0.7790 - val_loss: 0.6269 - val_acc: 0.7960\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6549 - acc: 0.7784 - val_loss: 0.6476 - val_acc: 0.7917\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6645 - acc: 0.7754 - val_loss: 0.6757 - val_acc: 0.7943\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6508 - acc: 0.7797 - val_loss: 0.6587 - val_acc: 0.7968\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6545 - acc: 0.7796 - val_loss: 0.6639 - val_acc: 0.7942\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6442 - acc: 0.7821 - val_loss: 0.6205 - val_acc: 0.8001\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6442 - acc: 0.7844 - val_loss: 0.6537 - val_acc: 0.7939\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6479 - acc: 0.7834 - val_loss: 0.7654 - val_acc: 0.7698\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6451 - acc: 0.7836 - val_loss: 0.6345 - val_acc: 0.7985\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6428 - acc: 0.7839 - val_loss: 0.6821 - val_acc: 0.7937\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6480 - acc: 0.7830 - val_loss: 0.6966 - val_acc: 0.7971\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6376 - acc: 0.7866 - val_loss: 0.6918 - val_acc: 0.7870\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6373 - acc: 0.7842 - val_loss: 0.6753 - val_acc: 0.7919\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 0.6427 - acc: 0.7839 - val_loss: 0.6594 - val_acc: 0.7950\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6366 - acc: 0.7875 - val_loss: 0.6942 - val_acc: 0.7908\n",
            "processing fold # 2\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 32s 26ms/step - loss: 1.7926 - acc: 0.3385 - val_loss: 1.4826 - val_acc: 0.4589\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.5193 - acc: 0.4521 - val_loss: 1.2378 - val_acc: 0.5693\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.3737 - acc: 0.5109 - val_loss: 1.2260 - val_acc: 0.5810\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.2768 - acc: 0.5521 - val_loss: 1.0777 - val_acc: 0.6201\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.2046 - acc: 0.5773 - val_loss: 1.0273 - val_acc: 0.6408\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1360 - acc: 0.6020 - val_loss: 0.9022 - val_acc: 0.6855\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0887 - acc: 0.6215 - val_loss: 0.8947 - val_acc: 0.6874\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0418 - acc: 0.6391 - val_loss: 0.8321 - val_acc: 0.7161\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0036 - acc: 0.6518 - val_loss: 0.8618 - val_acc: 0.7065\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9731 - acc: 0.6641 - val_loss: 0.8953 - val_acc: 0.7000\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9393 - acc: 0.6779 - val_loss: 0.8704 - val_acc: 0.7114\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9159 - acc: 0.6842 - val_loss: 0.9066 - val_acc: 0.7006\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8885 - acc: 0.6939 - val_loss: 0.7811 - val_acc: 0.7394\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8793 - acc: 0.6979 - val_loss: 0.8129 - val_acc: 0.7298\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8467 - acc: 0.7085 - val_loss: 0.7570 - val_acc: 0.7480\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8322 - acc: 0.7126 - val_loss: 0.7644 - val_acc: 0.7420\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8175 - acc: 0.7191 - val_loss: 0.7662 - val_acc: 0.7476\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8059 - acc: 0.7228 - val_loss: 0.7688 - val_acc: 0.7471\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7862 - acc: 0.7313 - val_loss: 0.8606 - val_acc: 0.7219\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7733 - acc: 0.7343 - val_loss: 0.6814 - val_acc: 0.7744\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7628 - acc: 0.7396 - val_loss: 0.6463 - val_acc: 0.7822\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7537 - acc: 0.7402 - val_loss: 0.6954 - val_acc: 0.7630\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7411 - acc: 0.7468 - val_loss: 0.9023 - val_acc: 0.7259\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7346 - acc: 0.7501 - val_loss: 0.6519 - val_acc: 0.7820\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7226 - acc: 0.7542 - val_loss: 0.7826 - val_acc: 0.7504\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7113 - acc: 0.7567 - val_loss: 0.6763 - val_acc: 0.7883\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7081 - acc: 0.7601 - val_loss: 0.6087 - val_acc: 0.7936\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6961 - acc: 0.7616 - val_loss: 0.6112 - val_acc: 0.8001\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6952 - acc: 0.7631 - val_loss: 0.6822 - val_acc: 0.7818\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6902 - acc: 0.7636 - val_loss: 0.6603 - val_acc: 0.7848\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6876 - acc: 0.7681 - val_loss: 0.6360 - val_acc: 0.7941\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6860 - acc: 0.7666 - val_loss: 0.6694 - val_acc: 0.7826\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6780 - acc: 0.7690 - val_loss: 0.6141 - val_acc: 0.8023\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6720 - acc: 0.7715 - val_loss: 0.6730 - val_acc: 0.7889\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6673 - acc: 0.7739 - val_loss: 0.5840 - val_acc: 0.8056\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6670 - acc: 0.7757 - val_loss: 0.6333 - val_acc: 0.7965\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6647 - acc: 0.7753 - val_loss: 0.6082 - val_acc: 0.7997\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6583 - acc: 0.7778 - val_loss: 0.5748 - val_acc: 0.8124\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6475 - acc: 0.7838 - val_loss: 0.6436 - val_acc: 0.7925\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6487 - acc: 0.7818 - val_loss: 0.7129 - val_acc: 0.7890\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6411 - acc: 0.7840 - val_loss: 0.8179 - val_acc: 0.7602\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6484 - acc: 0.7792 - val_loss: 0.7436 - val_acc: 0.7905\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6424 - acc: 0.7827 - val_loss: 0.6960 - val_acc: 0.7928\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 0.6415 - acc: 0.7852 - val_loss: 0.7067 - val_acc: 0.7785\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6403 - acc: 0.7845 - val_loss: 0.6877 - val_acc: 0.7841\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6394 - acc: 0.7833 - val_loss: 0.6516 - val_acc: 0.7916\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6332 - acc: 0.7865 - val_loss: 0.6749 - val_acc: 0.7977\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.6292 - acc: 0.7896 - val_loss: 0.7470 - val_acc: 0.7864\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 0.6303 - acc: 0.7885 - val_loss: 0.6425 - val_acc: 0.7978\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6342 - acc: 0.7906 - val_loss: 0.8159 - val_acc: 0.7702\n",
            "processing fold # 3\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.7987 - acc: 0.3390 - val_loss: 1.6591 - val_acc: 0.4048\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.5270 - acc: 0.4498 - val_loss: 1.4432 - val_acc: 0.4821\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.3787 - acc: 0.5088 - val_loss: 1.3507 - val_acc: 0.5329\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.2797 - acc: 0.5497 - val_loss: 1.1981 - val_acc: 0.5815\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.2029 - acc: 0.5765 - val_loss: 1.0735 - val_acc: 0.6256\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1377 - acc: 0.6032 - val_loss: 1.0110 - val_acc: 0.6440\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.0886 - acc: 0.6238 - val_loss: 0.9740 - val_acc: 0.6671\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0448 - acc: 0.6373 - val_loss: 0.9348 - val_acc: 0.6763\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 1.0087 - acc: 0.6477 - val_loss: 0.9814 - val_acc: 0.6650\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 0.9739 - acc: 0.6605 - val_loss: 0.8713 - val_acc: 0.7015\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 0.9459 - acc: 0.6724 - val_loss: 0.9183 - val_acc: 0.6810\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9160 - acc: 0.6843 - val_loss: 0.9078 - val_acc: 0.6901\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8960 - acc: 0.6921 - val_loss: 0.8727 - val_acc: 0.7027\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8692 - acc: 0.6996 - val_loss: 0.7580 - val_acc: 0.7420\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8521 - acc: 0.7052 - val_loss: 0.7801 - val_acc: 0.7357\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8367 - acc: 0.7115 - val_loss: 0.7588 - val_acc: 0.7496\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8190 - acc: 0.7179 - val_loss: 0.7412 - val_acc: 0.7523\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8079 - acc: 0.7231 - val_loss: 0.7634 - val_acc: 0.7441\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7932 - acc: 0.7262 - val_loss: 0.7437 - val_acc: 0.7557\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7746 - acc: 0.7323 - val_loss: 0.8369 - val_acc: 0.7354\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 0.7652 - acc: 0.7381 - val_loss: 0.7569 - val_acc: 0.7543\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7562 - acc: 0.7424 - val_loss: 0.8267 - val_acc: 0.7257\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7452 - acc: 0.7460 - val_loss: 0.8712 - val_acc: 0.7291\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7356 - acc: 0.7496 - val_loss: 0.6888 - val_acc: 0.7762\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7254 - acc: 0.7520 - val_loss: 0.7242 - val_acc: 0.7650\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7177 - acc: 0.7555 - val_loss: 0.7041 - val_acc: 0.7727\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7114 - acc: 0.7557 - val_loss: 0.7493 - val_acc: 0.7577\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7028 - acc: 0.7619 - val_loss: 0.6745 - val_acc: 0.7762\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6899 - acc: 0.7660 - val_loss: 0.7590 - val_acc: 0.7652\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6868 - acc: 0.7663 - val_loss: 0.7003 - val_acc: 0.7777\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6779 - acc: 0.7709 - val_loss: 0.7016 - val_acc: 0.7721\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6760 - acc: 0.7687 - val_loss: 0.6987 - val_acc: 0.7780\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6613 - acc: 0.7739 - val_loss: 0.7082 - val_acc: 0.7841\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6666 - acc: 0.7727 - val_loss: 0.7477 - val_acc: 0.7688\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6608 - acc: 0.7767 - val_loss: 0.6781 - val_acc: 0.7936\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6444 - acc: 0.7812 - val_loss: 0.6854 - val_acc: 0.7850\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6532 - acc: 0.7781 - val_loss: 0.6343 - val_acc: 0.7952\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6481 - acc: 0.7786 - val_loss: 0.6763 - val_acc: 0.7864\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6479 - acc: 0.7809 - val_loss: 0.7136 - val_acc: 0.7861\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6382 - acc: 0.7840 - val_loss: 0.7191 - val_acc: 0.7851\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6381 - acc: 0.7870 - val_loss: 0.6832 - val_acc: 0.7819\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6396 - acc: 0.7843 - val_loss: 0.6682 - val_acc: 0.7880\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6277 - acc: 0.7887 - val_loss: 0.6563 - val_acc: 0.7926\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6293 - acc: 0.7871 - val_loss: 0.6649 - val_acc: 0.7959\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6280 - acc: 0.7874 - val_loss: 0.7174 - val_acc: 0.7837\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6152 - acc: 0.7946 - val_loss: 0.6836 - val_acc: 0.7888\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6187 - acc: 0.7914 - val_loss: 0.6850 - val_acc: 0.7877\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6232 - acc: 0.7907 - val_loss: 0.6778 - val_acc: 0.7957\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6178 - acc: 0.7930 - val_loss: 0.7707 - val_acc: 0.7773\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6260 - acc: 0.7912 - val_loss: 0.7457 - val_acc: 0.7749\n",
            "processing fold # 4\n",
            "Epoch 1/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.8005 - acc: 0.3375 - val_loss: 1.5072 - val_acc: 0.4564\n",
            "Epoch 2/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.5339 - acc: 0.4505 - val_loss: 1.4093 - val_acc: 0.5061\n",
            "Epoch 3/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.3995 - acc: 0.5033 - val_loss: 1.2676 - val_acc: 0.5490\n",
            "Epoch 4/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.3041 - acc: 0.5408 - val_loss: 1.1458 - val_acc: 0.5938\n",
            "Epoch 5/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.2286 - acc: 0.5677 - val_loss: 1.0702 - val_acc: 0.6326\n",
            "Epoch 6/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1679 - acc: 0.5932 - val_loss: 0.9879 - val_acc: 0.6603\n",
            "Epoch 7/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.1155 - acc: 0.6079 - val_loss: 1.0170 - val_acc: 0.6541\n",
            "Epoch 8/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.0796 - acc: 0.6250 - val_loss: 0.9287 - val_acc: 0.6799\n",
            "Epoch 9/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 1.0376 - acc: 0.6410 - val_loss: 0.9135 - val_acc: 0.6842\n",
            "Epoch 10/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.9977 - acc: 0.6537 - val_loss: 0.9035 - val_acc: 0.6926\n",
            "Epoch 11/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9797 - acc: 0.6611 - val_loss: 0.8256 - val_acc: 0.7202\n",
            "Epoch 12/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9506 - acc: 0.6694 - val_loss: 0.8226 - val_acc: 0.7180\n",
            "Epoch 13/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9203 - acc: 0.6825 - val_loss: 0.8419 - val_acc: 0.7096\n",
            "Epoch 14/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9056 - acc: 0.6883 - val_loss: 0.8168 - val_acc: 0.7247\n",
            "Epoch 15/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8841 - acc: 0.6960 - val_loss: 0.7318 - val_acc: 0.7533\n",
            "Epoch 16/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8676 - acc: 0.7010 - val_loss: 0.7722 - val_acc: 0.7469\n",
            "Epoch 17/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8452 - acc: 0.7086 - val_loss: 0.7390 - val_acc: 0.7556\n",
            "Epoch 18/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8293 - acc: 0.7134 - val_loss: 0.7500 - val_acc: 0.7497\n",
            "Epoch 19/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8177 - acc: 0.7184 - val_loss: 0.7141 - val_acc: 0.7590\n",
            "Epoch 20/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7973 - acc: 0.7252 - val_loss: 0.7979 - val_acc: 0.7450\n",
            "Epoch 21/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7900 - acc: 0.7299 - val_loss: 0.7203 - val_acc: 0.7603\n",
            "Epoch 22/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7794 - acc: 0.7337 - val_loss: 0.7252 - val_acc: 0.7599\n",
            "Epoch 23/50\n",
            "1250/1250 [==============================] - 31s 24ms/step - loss: 0.7710 - acc: 0.7323 - val_loss: 0.6531 - val_acc: 0.7795\n",
            "Epoch 24/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7573 - acc: 0.7420 - val_loss: 0.6811 - val_acc: 0.7774\n",
            "Epoch 25/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7465 - acc: 0.7441 - val_loss: 0.7132 - val_acc: 0.7753\n",
            "Epoch 26/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7435 - acc: 0.7468 - val_loss: 0.7196 - val_acc: 0.7655\n",
            "Epoch 27/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7243 - acc: 0.7520 - val_loss: 0.7210 - val_acc: 0.7742\n",
            "Epoch 28/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7256 - acc: 0.7522 - val_loss: 0.6506 - val_acc: 0.7899\n",
            "Epoch 29/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7191 - acc: 0.7554 - val_loss: 0.6378 - val_acc: 0.7882\n",
            "Epoch 30/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.7143 - acc: 0.7565 - val_loss: 0.8828 - val_acc: 0.7337\n",
            "Epoch 31/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6982 - acc: 0.7612 - val_loss: 0.6436 - val_acc: 0.7961\n",
            "Epoch 32/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6975 - acc: 0.7631 - val_loss: 0.7107 - val_acc: 0.7791\n",
            "Epoch 33/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6891 - acc: 0.7651 - val_loss: 0.7067 - val_acc: 0.7813\n",
            "Epoch 34/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6818 - acc: 0.7687 - val_loss: 0.6712 - val_acc: 0.7895\n",
            "Epoch 35/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.6779 - acc: 0.7698 - val_loss: 0.7096 - val_acc: 0.7780\n",
            "Epoch 36/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6711 - acc: 0.7720 - val_loss: 0.6861 - val_acc: 0.7897\n",
            "Epoch 37/50\n",
            "1250/1250 [==============================] - 30s 24ms/step - loss: 0.6704 - acc: 0.7718 - val_loss: 0.6381 - val_acc: 0.7935\n",
            "Epoch 38/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6661 - acc: 0.7737 - val_loss: 0.6845 - val_acc: 0.7866\n",
            "Epoch 39/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6634 - acc: 0.7765 - val_loss: 0.6491 - val_acc: 0.7923\n",
            "Epoch 40/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6536 - acc: 0.7773 - val_loss: 0.7244 - val_acc: 0.7746\n",
            "Epoch 41/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6599 - acc: 0.7781 - val_loss: 0.6483 - val_acc: 0.7970\n",
            "Epoch 42/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6565 - acc: 0.7775 - val_loss: 0.6419 - val_acc: 0.7984\n",
            "Epoch 43/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6523 - acc: 0.7801 - val_loss: 0.6438 - val_acc: 0.7927\n",
            "Epoch 44/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6423 - acc: 0.7829 - val_loss: 0.6729 - val_acc: 0.7869\n",
            "Epoch 45/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6444 - acc: 0.7796 - val_loss: 0.7126 - val_acc: 0.7876\n",
            "Epoch 46/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6414 - acc: 0.7823 - val_loss: 0.6229 - val_acc: 0.8007\n",
            "Epoch 47/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6361 - acc: 0.7841 - val_loss: 0.6540 - val_acc: 0.8063\n",
            "Epoch 48/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6346 - acc: 0.7871 - val_loss: 0.6694 - val_acc: 0.7985\n",
            "Epoch 49/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6334 - acc: 0.7861 - val_loss: 0.6419 - val_acc: 0.8004\n",
            "Epoch 50/50\n",
            "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6347 - acc: 0.7861 - val_loss: 0.7052 - val_acc: 0.7833\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Geg5vP9am7-T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Successive mean K-fold validation scores**"
      ]
    },
    {
      "metadata": {
        "id": "H8scoTSkmqbB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "average_history = [\n",
        "    np.mean([x[i] for x in all_histories]) for i in range (50)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BG7W3Q10nsmN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Plotting validation scores**"
      ]
    },
    {
      "metadata": {
        "id": "D2QA_WrWnsRE",
        "colab_type": "code",
        "outputId": "bae2fc4e-7fd6-4279-8412-74fc118e20f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4611
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='training acc')\n",
        "plt.plot(range(1, len(average_history)+1),  average_history)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation\")\n",
        "plt.show()\n",
        "\n",
        "all_histories"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFYCAYAAAB6RnQAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkcQwvImIiCTm/wNzh87alWd+jgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.4916,\n",
              "  0.5284,\n",
              "  0.541,\n",
              "  0.6132,\n",
              "  0.6477,\n",
              "  0.6804,\n",
              "  0.6853,\n",
              "  0.7028,\n",
              "  0.6942,\n",
              "  0.7052,\n",
              "  0.7414,\n",
              "  0.7205,\n",
              "  0.7063,\n",
              "  0.7381,\n",
              "  0.7574,\n",
              "  0.7052,\n",
              "  0.7347,\n",
              "  0.7335,\n",
              "  0.7464,\n",
              "  0.7641,\n",
              "  0.7648,\n",
              "  0.7551,\n",
              "  0.7755,\n",
              "  0.7662,\n",
              "  0.7721,\n",
              "  0.7782,\n",
              "  0.7774,\n",
              "  0.7808,\n",
              "  0.769,\n",
              "  0.7736,\n",
              "  0.7831,\n",
              "  0.7544,\n",
              "  0.7624,\n",
              "  0.7595,\n",
              "  0.7667,\n",
              "  0.7869,\n",
              "  0.7935,\n",
              "  0.776,\n",
              "  0.7556,\n",
              "  0.7764,\n",
              "  0.7889,\n",
              "  0.8032,\n",
              "  0.7817,\n",
              "  0.7864,\n",
              "  0.782,\n",
              "  0.788,\n",
              "  0.7897,\n",
              "  0.7974,\n",
              "  0.7792,\n",
              "  0.7858],\n",
              " [0.4313,\n",
              "  0.5656,\n",
              "  0.6054,\n",
              "  0.5856,\n",
              "  0.6412,\n",
              "  0.662,\n",
              "  0.6732,\n",
              "  0.6835,\n",
              "  0.6988,\n",
              "  0.7079,\n",
              "  0.7034,\n",
              "  0.7361,\n",
              "  0.7403,\n",
              "  0.741,\n",
              "  0.7507,\n",
              "  0.7448,\n",
              "  0.765,\n",
              "  0.7603,\n",
              "  0.7592,\n",
              "  0.7676,\n",
              "  0.7728,\n",
              "  0.7627,\n",
              "  0.7668,\n",
              "  0.7685,\n",
              "  0.7906,\n",
              "  0.7916,\n",
              "  0.7616,\n",
              "  0.7875,\n",
              "  0.7781,\n",
              "  0.7662,\n",
              "  0.7757,\n",
              "  0.7791,\n",
              "  0.771,\n",
              "  0.7823,\n",
              "  0.7906,\n",
              "  0.796,\n",
              "  0.7917,\n",
              "  0.7943,\n",
              "  0.7968,\n",
              "  0.7942,\n",
              "  0.8001,\n",
              "  0.7939,\n",
              "  0.7698,\n",
              "  0.7985,\n",
              "  0.7937,\n",
              "  0.7971,\n",
              "  0.787,\n",
              "  0.7919,\n",
              "  0.795,\n",
              "  0.7908],\n",
              " [0.4589,\n",
              "  0.5693,\n",
              "  0.581,\n",
              "  0.6201,\n",
              "  0.6408,\n",
              "  0.6855,\n",
              "  0.6874,\n",
              "  0.7161,\n",
              "  0.7065,\n",
              "  0.7,\n",
              "  0.7114,\n",
              "  0.7006,\n",
              "  0.7394,\n",
              "  0.7298,\n",
              "  0.748,\n",
              "  0.742,\n",
              "  0.7476,\n",
              "  0.7471,\n",
              "  0.7219,\n",
              "  0.7744,\n",
              "  0.7822,\n",
              "  0.763,\n",
              "  0.7259,\n",
              "  0.782,\n",
              "  0.7504,\n",
              "  0.7883,\n",
              "  0.7936,\n",
              "  0.8001,\n",
              "  0.7818,\n",
              "  0.7848,\n",
              "  0.7941,\n",
              "  0.7826,\n",
              "  0.8023,\n",
              "  0.7889,\n",
              "  0.8056,\n",
              "  0.7965,\n",
              "  0.7997,\n",
              "  0.8124,\n",
              "  0.7925,\n",
              "  0.789,\n",
              "  0.7602,\n",
              "  0.7905,\n",
              "  0.7928,\n",
              "  0.7785,\n",
              "  0.7841,\n",
              "  0.7916,\n",
              "  0.7977,\n",
              "  0.7864,\n",
              "  0.7978,\n",
              "  0.7702],\n",
              " [0.4048,\n",
              "  0.4821,\n",
              "  0.5329,\n",
              "  0.5815,\n",
              "  0.6256,\n",
              "  0.644,\n",
              "  0.6671,\n",
              "  0.6763,\n",
              "  0.665,\n",
              "  0.7015,\n",
              "  0.681,\n",
              "  0.6901,\n",
              "  0.7027,\n",
              "  0.742,\n",
              "  0.7357,\n",
              "  0.7496,\n",
              "  0.7523,\n",
              "  0.7441,\n",
              "  0.7557,\n",
              "  0.7354,\n",
              "  0.7543,\n",
              "  0.7257,\n",
              "  0.7291,\n",
              "  0.7762,\n",
              "  0.765,\n",
              "  0.7727,\n",
              "  0.7577,\n",
              "  0.7762,\n",
              "  0.7652,\n",
              "  0.7777,\n",
              "  0.7721,\n",
              "  0.778,\n",
              "  0.7841,\n",
              "  0.7688,\n",
              "  0.7936,\n",
              "  0.785,\n",
              "  0.7952,\n",
              "  0.7864,\n",
              "  0.7861,\n",
              "  0.7851,\n",
              "  0.7819,\n",
              "  0.788,\n",
              "  0.7926,\n",
              "  0.7959,\n",
              "  0.7837,\n",
              "  0.7888,\n",
              "  0.7877,\n",
              "  0.7957,\n",
              "  0.7773,\n",
              "  0.7749],\n",
              " [0.4564,\n",
              "  0.5061,\n",
              "  0.549,\n",
              "  0.5938,\n",
              "  0.6326,\n",
              "  0.6603,\n",
              "  0.6541,\n",
              "  0.6799,\n",
              "  0.6842,\n",
              "  0.6926,\n",
              "  0.7202,\n",
              "  0.718,\n",
              "  0.7096,\n",
              "  0.7247,\n",
              "  0.7533,\n",
              "  0.7469,\n",
              "  0.7556,\n",
              "  0.7497,\n",
              "  0.759,\n",
              "  0.745,\n",
              "  0.7603,\n",
              "  0.7599,\n",
              "  0.7795,\n",
              "  0.7774,\n",
              "  0.7753,\n",
              "  0.7655,\n",
              "  0.7742,\n",
              "  0.7899,\n",
              "  0.7882,\n",
              "  0.7337,\n",
              "  0.7961,\n",
              "  0.7791,\n",
              "  0.7813,\n",
              "  0.7895,\n",
              "  0.778,\n",
              "  0.7897,\n",
              "  0.7935,\n",
              "  0.7866,\n",
              "  0.7923,\n",
              "  0.7746,\n",
              "  0.797,\n",
              "  0.7984,\n",
              "  0.7927,\n",
              "  0.7869,\n",
              "  0.7876,\n",
              "  0.8007,\n",
              "  0.8063,\n",
              "  0.7985,\n",
              "  0.8004,\n",
              "  0.7833]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "EtYzpQQb4DIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}