{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnitaKirkovska/MachineLearning_HW1/blob/master/Problem3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Oivtv2Jl2d0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f025899e-21d7-4aa4-ac9c-570a48ccdccd"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import scipy.special\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "from keras.utils import to_categorical\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "H-_FxRt028AD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Network model"
      ]
    },
    {
      "metadata": {
        "id": "wCoktQG_2524",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class neuralArch:\n",
        "  \n",
        "  def __init__ (self, inputnodes, outputnodes, learningrate):\n",
        "    self.inodes = inputnodes\n",
        "    self.onodes = outputnodes\n",
        "    \n",
        "    #link weight natrices. wio -> input and output\n",
        "    self.wio = np.random.normal(0.0, pow(self.inodes, -0.5), (self.onodes, self.inodes))\n",
        "    #learning rate\n",
        "    self.lr = learningrate\n",
        "    \n",
        "    #activation function -> softm\n",
        "    def softmax(x):\n",
        "      exp = np.exp(x)\n",
        "      sum_exp = np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "      softmax = exp / sum_exp\n",
        "      return softmax\n",
        "    \n",
        "    pass\n",
        "  \n",
        "  def train(self, inputs_list, targets_list):\n",
        "    \n",
        "    def softmax(x):\n",
        "      exp = np.exp(x)\n",
        "      sum_exp = np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "      softmax = exp / sum_exp\n",
        "      return softmax\n",
        "    \n",
        "    #convert to 2d array\n",
        "    inputs = np.array(inputs_list, ndmin=2).T\n",
        "    targets = np.array(targets_list, ndmin=2).T\n",
        "    \n",
        "    #calculating signals into last layer \n",
        "    last_inputs = np.dot(self.wio,inputs)\n",
        "    last_outputs = softmax(last_inputs)\n",
        "    \n",
        "    #output layer error categorical cross entropy loss\n",
        "    last_errors = -np.sum(targets * np.log(last_outputs))\n",
        "       \n",
        "    #update the weight links between input layers and last layers\n",
        "    self.wio += self.lr * np.dot((last_errors * last_outputs * (1.0 - last_outputs)), np.transpose(inputs))\n",
        "    \n",
        "    \n",
        "    pass\n",
        "  \n",
        "  \n",
        "  def query(self,inputs_list):\n",
        "    \n",
        "    def softmax(x):\n",
        "      exp = np.exp(x)\n",
        "      sum_exp = np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "      softmax = exp / sum_exp\n",
        "      return softmax\n",
        "    \n",
        "    #convert to 2d array\n",
        "    inputs = np.array(inputs_list, ndmin=2).T\n",
        "    \n",
        "    #calculating signals into and from last layer \n",
        "    last_inputs = np.dot(self.wio,inputs)\n",
        "    last_outputs = softmax(last_inputs)\n",
        "    print (last_outputs)\n",
        "    return last_outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ryVeBrZ13AQo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading the data"
      ]
    },
    {
      "metadata": {
        "id": "6mrmT9Bs29jF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train_org, y_train_org), (x_test_org, y_test_org) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DZ-Mwb3I3Dqz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preparing the data "
      ]
    },
    {
      "metadata": {
        "id": "VShAkA-M3BzP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = x_train_org.reshape((60000, 28 * 28))\n",
        "scaled_input_x = np.asfarray(x_train)/255.0\n",
        "\n",
        "x_test = x_test_org.reshape((10000, 28 * 28))\n",
        "scaled_test_x = np.asfarray(x_test)/255.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_q7cNd2q3E9z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#split into 10 classes with categorical labels\n",
        "\n",
        "scaled_input_x = scaled_input_x.reshape(scaled_input_x.shape[0], -1)\n",
        "scaled_test_x = scaled_test_x.reshape(scaled_test_x.shape[0], -1)\n",
        "y_train = to_categorical(y_train_org, num_classes=10) \n",
        "y_test = to_categorical(y_test_org, num_classes=10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zTbep-5csIRx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One hot model"
      ]
    },
    {
      "metadata": {
        "id": "ldBem_nVr3I4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#tranforms vector Y of labels to one-hot encoded matrix\n",
        "def one_hot(X, Y, num_class=10):\n",
        "  m = X.shape[0]\n",
        "  one_hot = np.zeros((m, num_class))\n",
        "  one_hot[np.arange(m), Y.T] = 1\n",
        "  return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mi_s3Tawr8u1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instance of the model"
      ]
    },
    {
      "metadata": {
        "id": "vzfgv7uv3QSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#output node is 1, and we have 10 classifiers with 28x28 input neurons\n",
        "input_nodes = len(scaled_input_x[0])\n",
        "output_nodes = len(y_train[0])\n",
        "\n",
        "learning_rate = 0.3\n",
        "\n",
        "#instance of the Network\n",
        "n = neuralArch(input_nodes, output_nodes, learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S3Jx3d7X3eC4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training the network"
      ]
    },
    {
      "metadata": {
        "id": "5vix07Md3aa5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "batch_size = 4\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  rand_ind = np.random.randint(0,epochs)\n",
        "  X_i = scaled_input_x[rand_ind,:].reshape(1,input_nodes)\n",
        "  Y_i = y_train_org[rand_ind].reshape(1,1)\n",
        "  y_one_hot = one_hot(X_i,Y_i)\n",
        "  #print(y_one_hot)\n",
        "  \n",
        "    #shuffled_indices = np.random.permutation(len(scaled_input_x))\n",
        "    #X_b_shuffled = scaled_input_x[shuffled_indices]\n",
        "    #y_shuffled = y_train_org[shuffled_indices]\n",
        "    #y_one_hot = one_hot(X_b_shi)\n",
        "    \n",
        "    \n",
        "  for i in tqdm(range(0, len(scaled_input_x), batch_size)):\n",
        "    xi = X_b_shuffled[i:i+batch_size]\n",
        "    yi = y_one_hot[i:i+batch_size]\n",
        "    #print (xi,yi.shape[0])\n",
        "    n.train(xi, y_one_hot)\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gO9Ia1S43j-7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Testing "
      ]
    },
    {
      "metadata": {
        "id": "r7OHF4Ko3hts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "answers = []\n",
        "for i in tqdm(range(len(scaled_test_x))):\n",
        "  answer = n.query(scaled_test_x[i])\n",
        "  answers.append(answer.argmax())\n",
        "  \n",
        "performance= []\n",
        "for i in answers:\n",
        "  if answers[i] == y_test.argmax():\n",
        "    performance.append(1)\n",
        "  else:\n",
        "    performance.append(0)\n",
        "    pass \n",
        "  \n",
        "performance_array = np.asarray(performance)\n",
        "print (\"performance = \", performance_array.sum() / performance_array.size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4pz2Z-Rpu4m2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n.query(scaled_test_x[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8gDLml-E3sAU",
        "colab_type": "code",
        "outputId": "d1888d7a-3c03-4330-8376-3a0d17e64597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "y_test[4].argmax()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "UQOhwLNcKqhd",
        "colab_type": "code",
        "outputId": "b44303cd-4830-4b27-aa82-2be3e80d1114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "answers[6]"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "metadata": {
        "id": "1HOqrcbGKy14",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}